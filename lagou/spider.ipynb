{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import re\n",
    "import time\n",
    "import mysql.connector\n",
    "import json\n",
    "import requests\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "urls= ['https://www.lagou.com/jobs/list_%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90?px=default&city=%E5%B9%BF%E5%B7%9E#filterBox',\n",
    "      'https://www.lagou.com/jobs/list_%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90?px=default&city=%E6%B7%B1%E5%9C%B3#filterBox']\n",
    "\n",
    "conn = mysql.connector.connect(user='name', password='', database='lagou')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute('''CREATE TABLE job_info (                                                                                                                          \n",
    "                num             int             NOT NULL AUTO_INCREMENT    PRIMARY KEY,\n",
    "                city            varchar(20)     NOT NULL,\n",
    "                pos_name        varchar(50)     NOT NULL,                                                                                                                        \n",
    "                company         varchar(50)     NOT NULL,                                                                                                                        \n",
    "                experience      varchar(20)     NOT NULL,                                                                                                                        \n",
    "                major           varchar(20)     NOT NULL,                                                                                                                        \n",
    "                salary          varchar(20)     NOT NULL,                                                                                                                        \n",
    "                industry        varchar(50)     NOT NULL,\n",
    "                `condition`     varchar(20)     NOT NULL,\n",
    "                job_features    varchar(100)    NOT NULL,\n",
    "                comp_features   varchar(100)    NOT NULL,\n",
    "                link            varchar(50)     NOT NULL,\n",
    "                size            varchar(20)         NULL, \n",
    "                address         varchar(100)        NULL,\n",
    "                `require`       varchar(2000)       NULL,\n",
    "                location        varchar(50)         NULL);''')\n",
    "\n",
    "\n",
    "re_tag = re.compile(u'con_list_item.*default_list')\n",
    "\n",
    "\n",
    "for url in urls:\n",
    "    options = webdriver.FirefoxOptions()\n",
    "    options.set_headless()\n",
    "    driver = webdriver.Firefox(firefox_options=options)\n",
    "    driver.get(url)\n",
    "    soup = bs(driver.page_source, 'lxml')\n",
    "    page_num = soup.find(class_='span totalNum').string\n",
    "    #for term in range(2):\n",
    "    for term in range(int(page_num)):\n",
    "        soup = bs(driver.page_source, 'lxml')\n",
    "        tags = soup.find_all('li', class_=re_tag)\n",
    "        for tag in tags:\n",
    "            city = soup.find('a', class_='current_city current').string\n",
    "            salary = tag['data-salary']\n",
    "            company = tag['data-company']\n",
    "            pos_name = tag['data-positionname']\n",
    "            experience = tag.find('div', class_='li_b_l').contents[4].strip().split(' / ')[0]\n",
    "            major = tag.find('div', class_='li_b_l').contents[4].strip().split(' / ')[1]\n",
    "            industry = tag.find('div', class_='industry').string.strip().split(' / ')[0]\n",
    "            condition = tag.find('div', class_='industry').string.strip().split(' / ')[1]\n",
    "            job_features = ','.join(list(tag.find('div', class_='list_item_bot').find_all('div')[0].stripped_strings))\n",
    "            comp_features = tag.find('div', class_='list_item_bot').find_all('div')[1].string.strip('“”')\n",
    "            link = tag.find('a', class_='position_link')['href']\n",
    "            \n",
    "            #然后存入数据库\n",
    "            cursor.execute('''INSERT INTO \n",
    "            job_info(pos_name, city, company, experience, major, salary, industry,\n",
    "            `condition`, job_features, comp_features, link)\n",
    "            VALUES(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)\n",
    "            '''%(repr(pos_name), repr(city), repr(company), repr(experience), repr(major), repr(salary), repr(industry),\n",
    "            repr(condition), repr(job_features), repr(comp_features), repr(link)))\n",
    "            \n",
    "            print(company)\n",
    "    \n",
    "        print('sleep')\n",
    "        driver.find_element_by_xpath(\"//ul[@id='order']/li/div[4]/div[2]\").click()\n",
    "        time.sleep(5)  #如果没有加载完全，现在获取html还是之前网的，我家网有毒，所以宁愿多等一下\n",
    "        print('-----------------------------------------------')\n",
    "    driver.close()\n",
    "            \n",
    "\n",
    "conn.commit()\n",
    "\n",
    "#cursor.execute('select link from job_info limit 5')\n",
    "cursor.execute('select link from job_info where address is null')\n",
    "pos_urls = cursor.fetchall()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for url in pos_urls:\n",
    "    #driver = webdriver.Firefox()\n",
    "    options = webdriver.FirefoxOptions()\n",
    "    options.set_headless()\n",
    "    driver = webdriver.Firefox(firefox_options=options)\n",
    "    driver.get(url[0])\n",
    "    soup = bs(driver.page_source, 'lxml')\n",
    "    size = soup.find('i',class_='icon-glyph-figure').next_sibling.strip()\n",
    "    address = ''.join(list(soup.find('div', class_='work_addr').stripped_strings)[:-1]).replace('-', '').replace(' ','')\n",
    "    require = ''.join(list(soup.find('dd', class_='job_bt').div.stripped_strings))\n",
    "    cursor.execute('''UPDATE job_info SET size=%s, address=%s, `require`=%s WHERE link=%s\n",
    "                   '''%(repr(size), repr(address), repr(require), repr(url[0])))\n",
    "    \n",
    "    print(size, '\\n', address, '\\n', require, '\\n\\n')\n",
    "    conn.commit()\n",
    "    #time.sleep(2)\n",
    "    driver.close()\n",
    "#conn.commit()\n",
    "#driver.close()\n",
    "\n",
    "\n",
    "cursor.execute('select address from job_info')\n",
    "\n",
    "address = cursor.fetchall()\n",
    "\n",
    "for addr in address:\n",
    "    url = 'https://restapi.amap.com/v3/geocode/geo?key=0d1ccccbc8a759ee19353a180c21723c&address=%s'%addr[0]\n",
    "    r = requests.get(url)\n",
    "    location = json.loads(r.content.decode('utf-8'))['geocodes'][0]['location']\n",
    "    cursor.execute('UPDATE job_info SET location=%s where address=%s'%(repr(location), repr(addr[0])))\n",
    "    print(location, addr[0])\n",
    "    time.sleep(2)\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "cursor.close()\n",
    "conn.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
